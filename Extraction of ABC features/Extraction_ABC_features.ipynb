{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59bbdc37-c1a3-472b-9a57-ca7e81f1f0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading target parameter table...\n",
      "Target file loaded: (91392, 7)\n",
      "Extracting planet IDs from filenames...\n",
      "Found 20000 planet files to process\n",
      "Planet ID range: 1 to 91389\n",
      "Processing 20000 spectral files...\n",
      "  Processed 1000 / 20000 planets\n",
      "  Processed 2000 / 20000 planets\n",
      "  Processed 3000 / 20000 planets\n",
      "  Processed 4000 / 20000 planets\n",
      "  Processed 5000 / 20000 planets\n",
      "  Processed 6000 / 20000 planets\n",
      "  Processed 7000 / 20000 planets\n",
      "  Processed 8000 / 20000 planets\n",
      "  Processed 9000 / 20000 planets\n",
      "  Processed 10000 / 20000 planets\n",
      "  Processed 11000 / 20000 planets\n",
      "  Processed 12000 / 20000 planets\n",
      "  Processed 13000 / 20000 planets\n",
      "  Processed 14000 / 20000 planets\n",
      "  Processed 15000 / 20000 planets\n",
      "  Processed 16000 / 20000 planets\n",
      "  Processed 17000 / 20000 planets\n",
      "  Processed 18000 / 20000 planets\n",
      "  Processed 19000 / 20000 planets\n",
      "  Processed 20000 / 20000 planets\n",
      "Concatenating all spectral features...\n",
      "Spectral matrix shape: (20000, 53)\n",
      "Merging with target parameters...\n",
      "Final merged matrix shape: (20000, 59)\n",
      "Saving final feature matrix to ABC_Feature_Matrix_Random.csv...\n",
      "SUCCESS: Feature matrix saved to ABC_Feature_Matrix_Random.csv\n",
      "Final dimensions: 20000 rows × 59 columns\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import gc\n",
    "import re\n",
    "\n",
    "def flatten_exoplanet_spectra(\n",
    "    data_dir=\"planet_data_random\",\n",
    "    target_file=\"FM_Parameter_Table.csv\",\n",
    "    output_file=\"ABC_Feature_Matrix.csv\",\n",
    "    batch_size=500\n",
    "):\n",
    "    \"\"\"\n",
    "    Flatten and vectorize exoplanet spectral data for randomly selected planets.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data_dir : str\n",
    "        Directory containing the spectral CSV files\n",
    "    target_file : str\n",
    "        Path to the target parameter file\n",
    "    output_file : str\n",
    "        Path for the output feature matrix\n",
    "    batch_size : int\n",
    "        Number of planets to process before concatenating (for memory efficiency)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the target parameter table\n",
    "    print(\"Loading target parameter table...\")\n",
    "    target_df = pd.read_csv(target_file)\n",
    "    target_df = target_df.rename(columns={'planet_ID': 'planet_ID'}) if 'planet_ID' in target_df.columns else target_df\n",
    "    print(f\"Target file loaded: {target_df.shape}\")\n",
    "    \n",
    "    # Extract planet IDs from filenames in the data directory\n",
    "    print(\"Extracting planet IDs from filenames...\")\n",
    "    data_path = Path(data_dir)\n",
    "    planet_files = list(data_path.glob(\"Planet_*_data.csv\"))\n",
    "    \n",
    "    # Extract planet IDs using regex pattern\n",
    "    planet_ids = []\n",
    "    for file_path in planet_files:\n",
    "        match = re.search(r'Planet_(\\d+)_data\\.csv', file_path.name)\n",
    "        if match:\n",
    "            planet_id = int(match.group(1))\n",
    "            planet_ids.append(planet_id)\n",
    "    \n",
    "    planet_ids.sort()  # Sort for consistent processing order\n",
    "    num_planets = len(planet_ids)\n",
    "    print(f\"Found {num_planets} planet files to process\")\n",
    "    print(f\"Planet ID range: {min(planet_ids)} to {max(planet_ids)}\")\n",
    "    \n",
    "    # Initialize list to collect batches\n",
    "    batch_list = []\n",
    "    \n",
    "    # Process planets in batches for memory efficiency\n",
    "    print(f\"Processing {num_planets} spectral files...\")\n",
    "    \n",
    "    for i, planet_id in enumerate(planet_ids):\n",
    "        # Construct file path\n",
    "        spectral_file = data_path / f\"Planet_{planet_id}_data.csv\"\n",
    "        \n",
    "        try:\n",
    "            # Load spectral data\n",
    "            spectrum_df = pd.read_csv(spectral_file)\n",
    "            \n",
    "            # Validate that required columns exist\n",
    "            required_cols = ['instrument_wlgrid', 'instrument_spectrum']\n",
    "            if not all(col in spectrum_df.columns for col in required_cols):\n",
    "                raise ValueError(f\"Missing required columns in {spectral_file}\")\n",
    "            \n",
    "            # CRUCIAL STEP: Sort by instrument_wlgrid in ascending order\n",
    "            spectrum_df = spectrum_df.sort_values(by='instrument_wlgrid', ascending=True)\n",
    "            \n",
    "            # Extract spectrum values after sorting (52 elements)\n",
    "            spectrum_values = spectrum_df['instrument_spectrum'].values\n",
    "            \n",
    "            # Validate length\n",
    "            if len(spectrum_values) != 52:\n",
    "                raise ValueError(f\"Expected 52 spectrum values, got {len(spectrum_values)}\")\n",
    "            \n",
    "            # Create feature names\n",
    "            feature_names = [f'spec_{i}' for i in range(52)]\n",
    "            \n",
    "            # Create a single-row DataFrame with planet_ID and spectral features\n",
    "            row_dict = {'planet_ID': planet_id}\n",
    "            row_dict.update({feature_names[i]: spectrum_values[i] for i in range(52)})\n",
    "            row_df = pd.DataFrame([row_dict])\n",
    "            \n",
    "            # Add to batch\n",
    "            batch_list.append(row_df)\n",
    "            \n",
    "            # Print progress\n",
    "            if (i + 1) % 1000 == 0:\n",
    "                print(f\"  Processed {i + 1} / {num_planets} planets\")\n",
    "        \n",
    "        except FileNotFoundError:\n",
    "            print(f\"WARNING: File not found: {spectral_file}\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR processing planet {planet_id}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Concatenate all batches into a single DataFrame\n",
    "    print(\"Concatenating all spectral features...\")\n",
    "    spectral_matrix = pd.concat(batch_list, ignore_index=True)\n",
    "    print(f\"Spectral matrix shape: {spectral_matrix.shape}\")\n",
    "    \n",
    "    # Clear memory\n",
    "    del batch_list\n",
    "    gc.collect()\n",
    "    \n",
    "    # Merge with target parameters using planet_ID as key\n",
    "    print(\"Merging with target parameters...\")\n",
    "    final_matrix = spectral_matrix.merge(\n",
    "        target_df,\n",
    "        on='planet_ID',\n",
    "        how='inner'\n",
    "    )\n",
    "    print(f\"Final merged matrix shape: {final_matrix.shape}\")\n",
    "    \n",
    "    # Save to output file\n",
    "    print(f\"Saving final feature matrix to {output_file}...\")\n",
    "    final_matrix.to_csv(output_file, index=False)\n",
    "    print(f\"SUCCESS: Feature matrix saved to {output_file}\")\n",
    "    print(f\"Final dimensions: {final_matrix.shape[0]} rows × {final_matrix.shape[1]} columns\")\n",
    "    \n",
    "    return final_matrix\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the pipeline with default parameters\n",
    "    # The function now automatically detects and processes only the random planet files\n",
    "    result = flatten_exoplanet_spectra(\n",
    "        data_dir=\"planet_data_random\",         # Directory with Planet_*.csv files\n",
    "        target_file=\"FM_Parameter_Table.csv\",  # Target parameters file\n",
    "        output_file=\"ABC_Feature_Matrix_Random.csv\",  # New output file name\n",
    "        batch_size=500                         # Process in batches for efficiency\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65c4c58-05ae-4045-bcda-98a019c36f38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
